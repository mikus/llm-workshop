#!/usr/bin/env node

import "dotenv/config";

import { ChatOpenAI } from "@langchain/openai";


const chat = new ChatOpenAI({
    apiKey: process.env.OPENAI_API_KEY,
    modelName: "gpt-4-turbo",
});



/*
### 5. Chain of thought concept

Chain of thought concept breaks down a problem into a series of intermediate reasoning steps. This way of thinking has significantly improved the quality of the outputs generated by the Large Language Models. 

Let's start with a simple problem. Although many problems may seem easy to us, some may be challenging to LLMs if they require solving intermediate steps before giving the final answer.
*/

var prompt = `Answer the following question.

Question: When I was three times younger, my sister was \
a year older than half of my age. \
Now, I’m 42. How old is my sister now?

Answer: `

var response = await chat.invoke(prompt);
console.log(response.content);

/*
The answer is __incorrect__! This is not a big surprise. Many Large Lanuage Models make these types of mistakes. In this case, the model skipped a few steps to solve the problem.
*/


/*
Let's try another idea. As we have seen in the __in-context__ learning topic, LLMs tend to learn from the provided inputs and apply those learnings to another problems. Here, we will first provide the step by step solution for the problem with different numbers and then ask the model to solve the original problem. 
*/

var prompt = `Answer the following question.

Question: When I was seven times younger, my sister was
2 years older than half of my age.
Now, I’m 70. How old is my sister now?

Answer: When I was seven times younger, my sister was \
2 years older than half of my age. \
So, my age at that wime was 70 / 7 = 10 years \
and the age of the sister at that time was 10 / 2 + 2 = 7. \
This implies that the sister is 3 years younger. \
Now, when I’m 70 years and age of sister is 70 - 3 \
Age of sister is 67.

Question: When I was three times younger, my sister was \
a year older than half of my age. \
Now I’m 42. How old is my sister now?

Answer: `

var response = await chat.invoke(prompt);
console.log(response.content);
