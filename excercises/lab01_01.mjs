#!/usr/bin/env node

import "dotenv/config";


/*
## What is LangChain?

> LangChain is a framework for developing applications powered by large language models (LLMs).

**TL;DR** LangChain makes the complicated parts of working & building with language models easier. It helps do this in two ways:

1. **Integration** - Bring external data, such as your files, other applications, and API data, to LLMs
2. **Agents** - Allows LLMs to interact with its environment via decision making and use LLMs to help decide which action to take next
*/

/*
 ### Why LangChain?

LangChain's modular implementation of components and common patterns combining these components makes it easier to build complex applications based on LLMs. LangChain enables these models to connect to data sources and systems as agents to take action.

1. **Components** are abstractions that works to bring external data, such as your documents, databases, applications,APIs to language models. LangChain makes it easy to swap out abstractions and components necessary to work with LLMs.

2. **Agents** enable language models to communicate with its environment, where the model then decides the next action to take. LangChain provides out of the box support for using and customizing 'chains' - a series of actions strung together.

Though LLMs can be straightforward (text-in, text-out) you'll quickly run into friction points that LangChain helps with once you develop more complicated applications.
 */

/*
## Using OpenAI for Inference

[OpenAI](https://openai.com/) provides access to state-of-the-art large language models (LLMs) and other foundational AI models that empower developers to create a variety of applications, from chatbots and content generation to advanced reasoning and problem-solving systems. With a focus on usability, OpenAI offers APIs that make it straightforward to integrate these models into your workflows, enabling developers to fine-tune, customize, or use pre-trained models for specific tasks.

The OpenAI platform prioritizes developer experience, scalability, and flexibility. Its models, including GPT-4 and its predecessors, are optimized for handling tasks of varying complexity. OpenAI also supports practical cost management by allowing developers to choose between different model sizes, reserving high-performance models for tasks requiring advanced capabilities while leveraging smaller models for simpler use cases.

In this workshop, we will primarily use LLMs through OpenAI APIs.

**Itâ€™s recommended to adopt cost-efficient practices when using OpenAI, such as using smaller models for straightforward tasks and reserving larger models for complex problem-solving.**

Learn more about [OpenAI pricing](https://openai.com/api/pricing/), and explore cost-effective usage patterns for your specific needs.
*/

import { ChatOpenAI } from "@langchain/openai";

const chat = new ChatOpenAI({
  apiKey: process.env.OPENAI_API_KEY,
  modelName: "gpt-4o-mini",
});

/*
**Try it Yourself!**

Try different prompts and observe the responses generated by the model.

Note: Results may not be factually accurate and may be based on false assumptions.
*/

var response = await chat.invoke("What is the capital of Spain?");
console.log(response.content);

